<!doctype html>
<html lang="fr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" type="image/png" href="assets/img/icon.png">
    <title>Research interests</title>
    <link rel="stylesheet" href="assets/css/style.css">
  </head>
  <body>
    <div class="wrapper">
      <div class="layout">
        <aside class="sidebar">
          <img class="photo" src="assets/img/photo.jpg" alt="Photo de Alya ZOUZOU">
        </aside>

        <section class="right">
          <div class="site-name">
          Alya ZOUZOU
          <span class="socials">
            <a href="https://github.com/alyasltd" target="_blank" aria-label="GitHub">
              <img src="assets/img/github.svg" alt="GitHub">
            </a>
            <a href="https://www.linkedin.com/in/alya-zouzou-827655255/" target="_blank" aria-label="LinkedIn">
              <img src="assets/img/linkedin.svg" alt="LinkedIn">
            </a>
            <a href="https://scholar.google.com/citations?user=TON_ID" target="_blank" aria-label="Google Scholar">
              <img src="assets/img/scholar.svg" alt="Google Scholar">
            </a>
          </span>
        </div>

          <nav aria-label="Navigation principale">
            <a class="nav-link" href="index.html">About</a>
            <a class="nav-link" href="cv.html">CV</a>
            <a class="nav-link" href="code.html">Code</a>
            <a class="nav-link active" href="research.html" aria-current="page">Research interests</a>
            <a class="nav-link" href="publications.html">Publications and posters</a>
            <a class="nav-link" href="blog/index.html">Blog</a>
            <a class="nav-link" href="contact.html">Contact</a>
          </nav>

          <main class="content">
  <h1>Research Interests</h1>

  <p>
    I’m broadly interested in how we can make AI systems that <strong>see, reason, and act</strong> more like humans — models that are <em>robust, fair, and aware of their own uncertainty</em>. 
    My focus lies at the intersection of <strong>computer vision</strong>, <strong>vision–language models</strong> (VLMs), and <strong>robotics</strong>.
  </p>

  <p>
    My previous work explored <strong>conformal prediction</strong>, <strong>object detection</strong>, and <strong>pose estimation</strong>, 
    under the supervision of <strong>Dr. Mélanie Ducoffe</strong>. 
    I also gained experience at <strong>PwC</strong> and <strong>Airbus AI Research</strong>, where I developed a deeper interest in 
    <strong>uncertainty quantification</strong> and <strong>trustworthy perception</strong> in real-world AI systems.
  </p>

  <p>
    I’m particularly drawn to problems involving <strong>vision–language reasoning</strong>, 
    <strong>fairness and robustness</strong> in perception models, 
    and <strong>active or causal learning</strong> for <strong>autonomous systems</strong>. 
    I find inspiration in robotics and medical AI applications, 
    where understanding context and reliability is crucial.
  </p>

  <p>
    My goal is to pursue a <strong>PhD</strong> or <strong>research assistantship</strong> 
    focused on developing <em>multimodal perception systems</em> 
    that combine vision, language, and reasoning — 
    making them not only accurate, but <strong>aware, interpretable, and dependable</strong>.
  </p>
</main>

          <footer>
            <div class="inner">
              <div>© <script>document.write(new Date().getFullYear())</script> Alya ZOUZOU </div>
              <div>Made with GitHub Pages</div>
            </div>
          </footer>
        </section>
      </div>
    </div>
  </body>
</html>